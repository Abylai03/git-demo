# Experiment with TableDiffusion-like Model for Indoor Localization

## Project Overview

This project experiments with a diffusion-like model, conceptually similar to TableDiffusion, for an indoor localization task. The primary goal is to generate synthetic Received Signal Strength Indicator (RSSI) data and evaluate its quality by comparing the performance of a K-Nearest Neighbors (KNN) regressor on both original and synthetic datasets for predicting X and Y coordinates.

## Dataset

The project utilizes an RSSI dataset collected from 11 Access Points (APs) using a Nexus 5 device.

* **Source File:** `RSSI_6AP_Experiments/Nexus5_Data/Database_Nexus5_11APs.csv`
* **Preprocessing Steps:**
    * The first row of the CSV is dropped.
    * Columns are renamed for clarity ('Unnamed: 0' to 'X', 'Unnamed: 1' to 'Y').
    * Object data types are converted to numeric types.
* **Data Shape:** The resulting DataFrame has a shape of (17270, 13).

A custom function `gen_test` is used to split the data, where a subset of data points (every 10th unique 'X' value, starting from the 10th) is selected as one part (`df1`), and the rest forms another part (`df_gen`).

## Methodology

The core of the project is a diffusion model implemented in PyTorch. This model is trained to learn the distribution of the RSSI data and then generate new, synthetic samples.

### Model Architecture

The diffusion model consists of the following key components:

1.  **`PositionalEncoding`**: Adds positional information to the input data, which is crucial for sequence-based models like Transformers. It uses sinusoidal functions of different frequencies.
2.  **`TransformerBlock`**: A standard Transformer encoder block, including multi-head self-attention and a feed-forward neural network. Layer normalization and dropout are used for regularization.
3.  **`Denoiser`**: This neural network (built using Transformer blocks) is the core of the diffusion process. It's trained to predict the noise that was added to the data at a particular timestep or to predict the original data from a noised version.
4.  **`DiffusionModel`**: This class orchestrates the diffusion process.
    * **Forward Process (Noising):** Gradually adds Gaussian noise to the input data over a sequence of timesteps.
    * **Reverse Process (Denoising/Sampling):** The trained `Denoiser` model iteratively removes noise from an initial random noise sample to generate synthetic data.

### Training

* The `DiffusionModel` is trained to minimize the Mean Squared Error (MSE) between the noise predicted by the `Denoiser` and the actual noise added during the forward process.
* The Adam optimizer is used.
* The training involves iterating through the dataset for a specified number of epochs, calculating the loss, and updating the model parameters.

### Synthetic Data Generation (Sampling)

After training, the `DiffusionModel` can generate synthetic data by starting with random noise and applying the reverse denoising process using the learned `Denoiser`.

### Evaluation

The quality of the generated synthetic data is evaluated by:
1.  Training a K-Nearest Neighbors (KNN) regressor (with k=5) to predict the 'X' and 'Y' coordinates using the RSSI features.
2.  This KNN model is trained and evaluated on:
    * The original dataset.
    * The synthetic dataset generated by the diffusion model.
3.  The Root Mean Squared Error (RMSE) is calculated for both scenarios to compare the localization accuracy.

## Key Steps in the Notebook

1.  **Import Libraries:** Essential libraries like `torch`, `numpy`, `pandas`, `sklearn`, and `matplotlib` are imported.
2.  **Data Loading and Preprocessing:** The RSSI dataset is loaded, cleaned, and preprocessed.
3.  **Data Splitting:** The `gen_test` function separates the data.
4.  **Feature Scaling:** `StandardScaler` from `sklearn` is used to scale the RSSI features and target coordinates.
5.  **Model Definition:** The `PositionalEncoding`, `TransformerBlock`, `Denoiser`, and `DiffusionModel` classes are defined.
6.  **Model Initialization and Training:** The `DiffusionModel` is instantiated, and the training loop is executed to train the `Denoiser`.
7.  **Synthetic Data Generation:** The trained model is used to generate synthetic RSSI samples.
8.  **Inverse Scaling:** The generated synthetic data and original data are inverse-transformed to their original scales.
9.  **KNN Evaluation:**
    * KNN is trained and tested on the original scaled data.
    * KNN is trained on the scaled synthetic data and tested on the original scaled test data.
    * RMSE scores are computed and printed for both.
10. **Visualization:** A scatter plot is generated to visualize the true locations against the locations predicted by KNN using both original and synthetic data.

## Libraries Used (Dependencies)

* `torch`
* `torch.nn`
* `torch.optim`
* `numpy`
* `pandas`
* `math`
* `torch.autograd.Variable`
* `torch.utils.data.DataLoader`, `torch.utils.data.Dataset`
* `sklearn.model_selection.train_test_split`
* `sklearn.preprocessing.StandardScaler`
* `sklearn.metrics.mean_squared_error`
* `sklearn.neighbors.KNeighborsRegressor`
* `matplotlib.pyplot`

## How to Run

1.  **Dataset:** Ensure the dataset file `Database_Nexus5_11APs.csv` is located in the `RSSI_6AP_Experiments/Nexus5_Data/` directory relative to the notebook, or update the path in the data loading cell.
2.  **Dependencies:** Install all the libraries listed above.
3.  **Execution:** Run the cells of the Jupyter Notebook sequentially.

## Expected Results and Visualizations

The notebook will:
* Train the diffusion model.
* Generate synthetic RSSI data.
* Print the Root Mean Squared Error (RMSE) for KNN predictions on:
    * The original dataset.
    * The synthetic dataset.
* Display a scatter plot titled "Indoor Localization: True vs Predicted Locations", showing:
    * True locations (yellow 'x' markers).
    * Locations predicted by KNN using the original data (blue markers).
    * Locations predicted by KNN using the synthetic data (red markers).

This visualization and the RMSE scores help in assessing how well the diffusion model captures the underlying data distribution for the indoor localization task.
